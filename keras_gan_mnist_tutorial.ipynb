{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Define channel order, module"
   ],
   "metadata": {
    "id": "UZqDpAqN5zQZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras import layers, models, optimizers\r\n",
    "from tensorflow.keras import datasets\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "K.set_image_data_format('channels_first')\r\n",
    "print(K.image_data_format())\r\n",
    "\r\n",
    "\r\n",
    "class GAN(models.Sequential):\r\n",
    "    def __init__(self, input_dim):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.input_dim = input_dim\r\n",
    "\r\n",
    "        self.generator = self.make_G()\r\n",
    "        self.discriminator = self.make_D()\r\n",
    "\r\n",
    "        self.add(self.generator)\r\n",
    "        self.discriminator.trainable = False\r\n",
    "        self.add(self.discriminator)\r\n",
    "\r\n",
    "        self.compile_all()\r\n",
    "\r\n",
    "    def make_G(self):\r\n",
    "        input_dim = self.input_dim\r\n",
    "\r\n",
    "        model = models.Sequential()\r\n",
    "        model.add(layers.Dense(1024, activation='tanh', input_dim=input_dim))\r\n",
    "        model.add(layers.Dense(128 * 7 * 7, activation='tanh'))\r\n",
    "        model.add(layers.BatchNormalization())\r\n",
    "        model.add(layers.Reshape((128, 7, 7), input_shape=(128 * 7 * 7, )))\r\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\r\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh'))\r\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\r\n",
    "        model.add(layers.Conv2D(1, (5, 5), padding='same', activation='tanh'))\r\n",
    "        return model\r\n",
    "\r\n",
    "    def make_D(self):\r\n",
    "        model = models.Sequential()\r\n",
    "        model.add(\r\n",
    "            layers.Conv2D(64, (5, 5),\r\n",
    "                          padding='same',\r\n",
    "                          activation='tanh',\r\n",
    "                          input_shape=(1, 28, 28)))\r\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
    "        model.add(layers.Conv2D(128, (5, 5), padding='same',\r\n",
    "                                activation='tanh'))\r\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
    "        model.add(layers.Flatten())\r\n",
    "        model.add(layers.Dense(1024, activation='tanh'))\r\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\r\n",
    "        return model\r\n",
    "\r\n",
    "    def compile_all(self):\r\n",
    "        opt_D = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\r\n",
    "        opt_G = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\r\n",
    "\r\n",
    "        self.compile(loss='binary_crossentropy', optimizer=opt_G)\r\n",
    "\r\n",
    "        self.discriminator.trainable = True\r\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=opt_D)\r\n",
    "\r\n",
    "    def get_z(self, ln):\r\n",
    "        return np.random.uniform(-1.0, 1.0, (ln, self.input_dim))\r\n",
    "\r\n",
    "    def train_once(self, x):\r\n",
    "        ln = x.shape[0]\r\n",
    "\r\n",
    "        z = self.get_z(ln)\r\n",
    "        gen = self.generator.predict(z, verbose=0)\r\n",
    "        input_D = np.concatenate((x, gen))\r\n",
    "        y_D = [1] * ln + [0] * ln\r\n",
    "        loss_D = self.discriminator.train_on_batch(input_D, y_D)\r\n",
    "\r\n",
    "        z = self.get_z(ln)\r\n",
    "        self.discriminator.trainable = False\r\n",
    "        loss_G = self.train_on_batch(z, [1] * ln)\r\n",
    "        self.discriminator.trainable = True\r\n",
    "\r\n",
    "        return loss_D, loss_G\r\n",
    "\r\n",
    "\r\n",
    "def get_x(x_train, index, batch_size):\r\n",
    "    return x_train[index * batch_size:(index + 1) * batch_size]\r\n",
    "\r\n",
    "\r\n",
    "class MnistData():\r\n",
    "    def __init__(self):\r\n",
    "        (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\r\n",
    "\r\n",
    "        img_rows, img_cols = x_train.shape[1:]\r\n",
    "\r\n",
    "        x_train = x_train.astype('float32') - 127.5\r\n",
    "        x_test = x_test.astype('float32') - 127.5\r\n",
    "        x_train /= 127.5\r\n",
    "        x_test /= 127.5\r\n",
    "\r\n",
    "        self.num_classes = 10\r\n",
    "        self.x_train, self.y_train = x_train, y_train\r\n",
    "        self.x_test, self.y_test = x_test, y_test\r\n",
    "\r\n",
    "\r\n",
    "def main():\r\n",
    "    batch_size = 100\r\n",
    "    epochs = 30\r\n",
    "    input_dim = 100\r\n",
    "    sample_size = 6\r\n",
    "\r\n",
    "    data = MnistData()\r\n",
    "    x_train = data.x_train\r\n",
    "    x_train = x_train.reshape((x_train.shape[0], 1) + x_train.shape[1:])\r\n",
    "\r\n",
    "    gan = GAN(input_dim)\r\n",
    "\r\n",
    "    for epoch in range(epochs):\r\n",
    "        print(\"Epoch\", epoch)\r\n",
    "\r\n",
    "        for index in range(int(x_train.shape[0] / batch_size)):\r\n",
    "            x = get_x(x_train, index, batch_size)\r\n",
    "            loss_D, loss_G = gan.train_once(x)\r\n",
    "\r\n",
    "        print('Loss D:', loss_D)\r\n",
    "        print('Loss G:', loss_G)\r\n",
    "\r\n",
    "        if epoch % 2 == 0 or epoch == epochs - 1:\r\n",
    "            z = gan.get_z(sample_size)\r\n",
    "            gen = gan.generator.predict(z, verbose=0)\r\n",
    "\r\n",
    "            plt.figure(figsize=(20, 2))\r\n",
    "\r\n",
    "            for i in range(sample_size):\r\n",
    "                ax = plt.subplot(1, sample_size, i + 1)\r\n",
    "                plt.imshow(gen[i].reshape((28, 28)))\r\n",
    "                ax.get_xaxis().set_visible(False)\r\n",
    "                ax.get_yaxis().set_visible(False)\r\n",
    "\r\n",
    "            plt.show()\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    main()\r\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T10:12:09.469396Z",
     "start_time": "2021-05-20T10:12:08.057579Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import argparse\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from keras.models import Model, Sequential\r\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\r\n",
    "from keras.layers.advanced_activations import LeakyReLU\r\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\r\n",
    "from keras.layers.normalization import BatchNormalization\r\n",
    "from keras.datasets import mnist\r\n",
    "from keras.optimizers import Adam\r\n",
    "from keras import initializers\r\n",
    "from keras import backend as K\r\n",
    "\r\n",
    "K.set_image_data_format('channels_first')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T04:03:37.902752Z",
     "start_time": "2021-05-20T04:03:37.887792Z"
    },
    "id": "NBmv_OBr5m8O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Dataset"
   ],
   "metadata": {
    "id": "8LTA95-D58gg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Data:\r\n",
    "    \"\"\"\r\n",
    "    Define dataset for training GAN\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, batch_size, z_input_dim):\r\n",
    "        # load mnist dataset\r\n",
    "        # 이미지는 보통 -1~1 사이의 값으로 normalization : generator의 outputlayer를 tanh로\r\n",
    "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n",
    "        self.x_data = ((X_train.astype(np.float32) - 127.5) / 127.5)\r\n",
    "        self.x_data = self.x_data.reshape((self.x_data.shape[0], 1) +\r\n",
    "                                          self.x_data.shape[1:])\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.z_input_dim = z_input_dim\r\n",
    "\r\n",
    "    def get_real_sample(self):\r\n",
    "        \"\"\"\r\n",
    "        get real sample mnist images\r\n",
    "\r\n",
    "        :return: batch_size number of mnist image data\r\n",
    "        \"\"\"\r\n",
    "        return self.x_data[np.random.randint(0,\r\n",
    "                                             self.x_data.shape[0],\r\n",
    "                                             size=self.batch_size)]\r\n",
    "\r\n",
    "    def get_z_sample(self, sample_size):\r\n",
    "        \"\"\"\r\n",
    "        get z sample data\r\n",
    "\r\n",
    "        :return: random z data (batch_size, z_input_dim) size\r\n",
    "        \"\"\"\r\n",
    "        return np.random.uniform(-1.0, 1.0, (sample_size, self.z_input_dim))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T01:27:24.709794Z",
     "start_time": "2021-05-20T01:27:24.697827Z"
    },
    "id": "_C4wa1p35-3s"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Mnist data test"
   ],
   "metadata": {
    "id": "5Uu0cTJq6FeX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = Data(batch_size=2, z_input_dim=10)\r\n",
    "print(data.get_real_sample())\r\n",
    "print(data.get_z_sample(2))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.403439Z",
     "start_time": "2021-05-19T15:32:56.421Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "tP3VXE176IJx",
    "outputId": "de18ee3d-3292-4a12-c516-2e66980c5913"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GAN 모델 정의\n",
    "\n",
    "- discriminator : CNN 판별기로 모델링\n",
    "- generator : input Z를 확장한 뒤 CNN 생성기로 모델링"
   ],
   "metadata": {
    "id": "WBttxOqW62BX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class GAN:\r\n",
    "    def __init__(self, learning_rate, z_input_dim):\r\n",
    "        \"\"\"\r\n",
    "        init params\r\n",
    "\r\n",
    "        :param learning_rate: learning rate of optimizer\r\n",
    "        :param z_input_dim: input dim of z\r\n",
    "        \"\"\"\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.z_input_dim = z_input_dim\r\n",
    "        self.D = self.discriminator()\r\n",
    "        self.G = self.generator()\r\n",
    "        self.GD = self.combined()\r\n",
    "\r\n",
    "    def discriminator(self):\r\n",
    "        \"\"\"\r\n",
    "        define discriminator\r\n",
    "        \"\"\"\r\n",
    "        D = Sequential()\r\n",
    "        D.add(\r\n",
    "            Conv2D(256, (5, 5),\r\n",
    "                   padding='same',\r\n",
    "                   input_shape=(1, 28, 28),\r\n",
    "                   kernel_initializer=initializers.RandomNormal(stddev=0.02)))\r\n",
    "        D.add(LeakyReLU(0.2))\r\n",
    "        D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\r\n",
    "        D.add(Dropout(0.3))\r\n",
    "        D.add(Conv2D(512, (5, 5), padding='same'))\r\n",
    "        D.add(LeakyReLU(0.2))\r\n",
    "        D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\r\n",
    "        D.add(Dropout(0.3))\r\n",
    "        D.add(Flatten())\r\n",
    "        D.add(Dense(256))\r\n",
    "        D.add(LeakyReLU(0.2))\r\n",
    "        D.add(Dropout(0.3))\r\n",
    "        D.add(Dense(1, activation='sigmoid'))\r\n",
    "\r\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\r\n",
    "        D.compile(loss='binary_crossentropy',\r\n",
    "                  optimizer=adam,\r\n",
    "                  metrics=['accuracy'])\r\n",
    "        return D\r\n",
    "\r\n",
    "    def generator(self):\r\n",
    "        \"\"\"\r\n",
    "        define generator\r\n",
    "        \"\"\"\r\n",
    "        G = Sequential()\r\n",
    "        G.add(Dense(512, input_dim=self.z_input_dim))\r\n",
    "        G.add(LeakyReLU(0.2))\r\n",
    "        G.add(Dense(128 * 7 * 7))\r\n",
    "        G.add(LeakyReLU(0.2))\r\n",
    "        G.add(BatchNormalization())\r\n",
    "        G.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7, )))\r\n",
    "        G.add(UpSampling2D(size=(2, 2)))\r\n",
    "        G.add(Conv2D(64, (5, 5), padding='same', activation='tanh'))\r\n",
    "        G.add(UpSampling2D(size=(2, 2)))\r\n",
    "        G.add(Conv2D(1, (5, 5), padding='same', activation='tanh'))\r\n",
    "\r\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\r\n",
    "        G.compile(loss='binary_crossentropy',\r\n",
    "                  optimizer=adam,\r\n",
    "                  metrics=['accuracy'])\r\n",
    "        return G\r\n",
    "\r\n",
    "    def combined(self):\r\n",
    "        \"\"\"\r\n",
    "        defien combined gan model\r\n",
    "        \"\"\"\r\n",
    "        G, D = self.G, self.D\r\n",
    "        D.trainable = False\r\n",
    "        GD = Sequential()\r\n",
    "        GD.add(G)\r\n",
    "        GD.add(D)\r\n",
    "\r\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\r\n",
    "        GD.compile(loss='binary_crossentropy',\r\n",
    "                   optimizer=adam,\r\n",
    "                   metrics=['accuracy'])\r\n",
    "        D.trainable = True\r\n",
    "        return GD"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.403439Z",
     "start_time": "2021-05-19T15:32:56.424Z"
    },
    "id": "wbWs_j2I6YqR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learner 구성\n",
    "\n",
    "- discriminator와 generator를 다른 epoch 비율로 학습 가능하도록 구성\n",
    "- 20 epoch 마다 이미지 생성\n",
    "- D, G를 각각 학습\n",
    "- 학습 완료 후 loss graph 생성"
   ],
   "metadata": {
    "id": "GjzV3EMG7J91"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Model:\r\n",
    "    def __init__(self, batch_size, epochs, learning_rate, z_input_dim,\r\n",
    "                 n_iter_D, n_iter_G):\r\n",
    "        self.epochs = epochs\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.z_input_dim = z_input_dim\r\n",
    "        self.data = Data(self.batch_size, self.z_input_dim)\r\n",
    "\r\n",
    "        # the reason why D, G differ in iter : Generator needs more training than Discriminator\r\n",
    "        self.n_iter_D = n_iter_D\r\n",
    "        self.n_iter_G = n_iter_G\r\n",
    "        self.gan = GAN(self.learning_rate, self.z_input_dim)\r\n",
    "\r\n",
    "        # print status\r\n",
    "        batch_count = self.data.x_data.shape[0] / self.batch_size\r\n",
    "        print('Epochs:', self.epochs)\r\n",
    "        print('Batch size:', self.batch_size)\r\n",
    "        print('Batches per epoch:', batch_count)\r\n",
    "        print('Learning rate:', self.learning_rate)\r\n",
    "        print('Image data format:', K.image_data_format())\r\n",
    "\r\n",
    "    def fit(self):\r\n",
    "        self.d_loss = []\r\n",
    "        self.g_loss = []\r\n",
    "        for epoch in range(self.epochs):\r\n",
    "\r\n",
    "            # train discriminator by real data\r\n",
    "            dloss = 0\r\n",
    "            for iter in range(self.n_iter_D):\r\n",
    "                dloss = self.train_D()\r\n",
    "\r\n",
    "            # train GD by generated fake data\r\n",
    "            gloss = 0\r\n",
    "            for iter in range(self.n_iter_G):\r\n",
    "                gloss = self.train_G()\r\n",
    "\r\n",
    "            # save loss data\r\n",
    "            self.d_loss.append(dloss)\r\n",
    "            self.g_loss.append(gloss)\r\n",
    "\r\n",
    "            # plot and save model each 20n epoch\r\n",
    "            if epoch % 20 == 0:\r\n",
    "                self.plot_generate_images(epoch, self.gan.G, examples=8)\r\n",
    "                print('Epoch:', str(epoch))\r\n",
    "                print('Discriminator loss:', str(dloss))\r\n",
    "                print('Generator loss:', str(gloss))\r\n",
    "\r\n",
    "        # show loss after train\r\n",
    "        self.plot_loss_graph(self.g_loss, self.d_loss)\r\n",
    "\r\n",
    "    def train_D(self):\r\n",
    "        \"\"\"\r\n",
    "        train Discriminator\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        # Real data\r\n",
    "        real = self.data.get_real_sample()\r\n",
    "\r\n",
    "        # Generated data\r\n",
    "        z = self.data.get_z_sample(self.batch_size)\r\n",
    "        generated_images = self.gan.G.predict(z)\r\n",
    "\r\n",
    "        # labeling and concat generated, real images\r\n",
    "        x = np.concatenate((real, generated_images), axis=0)\r\n",
    "        y = [0.9] * self.batch_size + [0] * self.batch_size\r\n",
    "\r\n",
    "        # train discriminator\r\n",
    "        self.gan.D.trainable = True\r\n",
    "        loss = self.gan.D.train_on_batch(x, y)\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def train_G(self):\r\n",
    "        \"\"\"\r\n",
    "        train Generator\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        # Generated data\r\n",
    "        z = self.data.get_z_sample(self.batch_size)\r\n",
    "\r\n",
    "        # labeling\r\n",
    "        y = [1] * self.batch_size\r\n",
    "\r\n",
    "        # train generator\r\n",
    "        self.gan.D.trainable = False\r\n",
    "        loss = self.gan.GD.train_on_batch(z, y)\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def plot_loss_graph(self, g_loss, d_loss):\r\n",
    "        \"\"\"\r\n",
    "        Save training loss graph\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        # show loss graph\r\n",
    "        plt.figure(figsize=(10, 8))\r\n",
    "        plt.plot(d_loss, label='Discriminator loss')\r\n",
    "        plt.plot(g_loss, label='Generator loss')\r\n",
    "        plt.xlabel('Epoch')\r\n",
    "        plt.ylabel('Loss')\r\n",
    "        plt.legend()\r\n",
    "        plt.show()\r\n",
    "\r\n",
    "    def plot_generate_images(self, epoch, generator, examples=8):\r\n",
    "        \"\"\"\r\n",
    "        Save generated mnist images\r\n",
    "        \"\"\"\r\n",
    "        # plt info\r\n",
    "        dim = (10, 10)\r\n",
    "        figsize = (10, 10)\r\n",
    "\r\n",
    "        # generate images\r\n",
    "        z = self.data.get_z_sample(examples)\r\n",
    "        generated_images = generator.predict(z)\r\n",
    "\r\n",
    "        # show images\r\n",
    "        plt.figure(figsize=figsize)\r\n",
    "        for i in range(generated_images.shape[0]):\r\n",
    "            plt.subplot(dim[0], dim[1], i + 1)\r\n",
    "            plt.imshow(generated_images[i].reshape((28, 28)),\r\n",
    "                       interpolation='nearest',\r\n",
    "                       cmap='gray_r')\r\n",
    "            plt.axis('off')\r\n",
    "        plt.tight_layout()\r\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.404436Z",
     "start_time": "2021-05-19T15:32:56.426Z"
    },
    "id": "UcZP9H6b7FFF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 학습 진행"
   ],
   "metadata": {
    "id": "LnwetMR87-Za"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def main():\r\n",
    "    # set hyper parameters\r\n",
    "    batch_size = 128\r\n",
    "    epochs = 1000\r\n",
    "    learning_rate = 0.0002\r\n",
    "    z_input_dim = 100\r\n",
    "    n_iter_D = 1\r\n",
    "    n_iter_G = 5\r\n",
    "\r\n",
    "    # run model\r\n",
    "    model = Model(batch_size, epochs, learning_rate, z_input_dim, n_iter_D,\r\n",
    "                  n_iter_G)\r\n",
    "    model.fit()\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    main()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.405435Z",
     "start_time": "2021-05-19T15:32:56.429Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7807
    },
    "id": "U2mIyr0c75-Y",
    "outputId": "1136e27e-6f75-4315-ff4f-264ce09c9745"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 시각화"
   ],
   "metadata": {
    "id": "NVhN5VeLCf9w"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import SVG\r\n",
    "from keras.utils.vis_utils import model_to_dot"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.405435Z",
     "start_time": "2021-05-19T15:32:56.431Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2985
    },
    "id": "WUPFmJsp8AFj",
    "outputId": "f2d7f40e-2c48-4152-f2e3-aa881212361d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "D = Sequential()\r\n",
    "D.add(Conv2D(256, (5, 5),\r\n",
    "         padding='same',\r\n",
    "         input_shape=(1, 28, 28),\r\n",
    "         kernel_initializer=initializers.RandomNormal(stddev=0.02)))\r\n",
    "D.add(LeakyReLU(0.2))\r\n",
    "D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\r\n",
    "D.add(Dropout(0.3))\r\n",
    "D.add(Conv2D(512, (5, 5), padding='same'))\r\n",
    "D.add(LeakyReLU(0.2))\r\n",
    "D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\r\n",
    "D.add(Dropout(0.3))\r\n",
    "D.add(Flatten())\r\n",
    "D.add(Dense(256))\r\n",
    "D.add(LeakyReLU(0.2))\r\n",
    "D.add(Dropout(0.3))\r\n",
    "D.add(Dense(1, activation='sigmoid'))\r\n",
    "\r\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\r\n",
    "D.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "SVG(model_to_dot(D, show_shapes=True).create(prog='dot', format='svg'))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.406433Z",
     "start_time": "2021-05-19T15:32:56.434Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1532
    },
    "id": "6b8MYOq9BduD",
    "outputId": "bca5818c-42a6-4137-8cdb-b23b20ee646a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "G = Sequential()\r\n",
    "G.add(Dense(512, input_dim=100))\r\n",
    "G.add(LeakyReLU(0.2))\r\n",
    "G.add(Dense(128 * 7 * 7))\r\n",
    "G.add(LeakyReLU(0.2))\r\n",
    "G.add(BatchNormalization())\r\n",
    "G.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7, )))\r\n",
    "G.add(UpSampling2D(size=(2, 2)))\r\n",
    "G.add(Conv2D(64, (5, 5), padding='same', activation='tanh'))\r\n",
    "G.add(UpSampling2D(size=(2, 2)))\r\n",
    "G.add(Conv2D(1, (5, 5), padding='same', activation='tanh'))\r\n",
    "\r\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\r\n",
    "G.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\r\n",
    "\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "SVG(model_to_dot(G, show_shapes=True).create(prog='dot', format='svg'))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.407429Z",
     "start_time": "2021-05-19T15:32:56.436Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1200
    },
    "id": "VHFXDUjABono",
    "outputId": "6b9573c7-f906-4264-85ec-cd74c825d446"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "keras-gan-mnist-tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}