{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZqDpAqN5zQZ"
   },
   "source": [
    "## Define channel order, module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T17:26:27.029220Z",
     "start_time": "2021-05-19T17:26:27.006282Z"
    },
    "id": "NBmv_OBr5m8O"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_config' from 'tensorflow.python.eager.context' (C:\\Users\\DevGun\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\context.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9ba2e645022c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvanced_activations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_coordinator\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_coordinator_context\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdc_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_config' from 'tensorflow.python.eager.context' (C:\\Users\\DevGun\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\context.py)"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LTA95-D58gg"
   },
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.402442Z",
     "start_time": "2021-05-19T15:32:56.419Z"
    },
    "id": "_C4wa1p35-3s"
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \"\"\"\n",
    "    Define dataset for training GAN\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, z_input_dim):\n",
    "        # load mnist dataset\n",
    "        # 이미지는 보통 -1~1 사이의 값으로 normalization : generator의 outputlayer를 tanh로\n",
    "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "        self.x_data = ((X_train.astype(np.float32) - 127.5) / 127.5)\n",
    "        self.x_data = self.x_data.reshape((self.x_data.shape[0], 1) +\n",
    "                                          self.x_data.shape[1:])\n",
    "        self.batch_size = batch_size\n",
    "        self.z_input_dim = z_input_dim\n",
    "\n",
    "    def get_real_sample(self):\n",
    "        \"\"\"\n",
    "        get real sample mnist images\n",
    "\n",
    "        :return: batch_size number of mnist image data\n",
    "        \"\"\"\n",
    "        return self.x_data[np.random.randint(0,\n",
    "                                             self.x_data.shape[0],\n",
    "                                             size=self.batch_size)]\n",
    "\n",
    "    def get_z_sample(self, sample_size):\n",
    "        \"\"\"\n",
    "        get z sample data\n",
    "\n",
    "        :return: random z data (batch_size, z_input_dim) size\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-1.0, 1.0, (sample_size, self.z_input_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uu0cTJq6FeX"
   },
   "source": [
    "- Mnist data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.403439Z",
     "start_time": "2021-05-19T15:32:56.421Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "tP3VXE176IJx",
    "outputId": "de18ee3d-3292-4a12-c516-2e66980c5913"
   },
   "outputs": [],
   "source": [
    "data = Data(batch_size=2, z_input_dim=10)\n",
    "print(data.get_real_sample())\n",
    "print(data.get_z_sample(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBttxOqW62BX"
   },
   "source": [
    "## GAN 모델 정의\n",
    "\n",
    "- discriminator : CNN 판별기로 모델링\n",
    "- generator : input Z를 확장한 뒤 CNN 생성기로 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.403439Z",
     "start_time": "2021-05-19T15:32:56.424Z"
    },
    "id": "wbWs_j2I6YqR"
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, learning_rate, z_input_dim):\n",
    "        \"\"\"\n",
    "        init params\n",
    "\n",
    "        :param learning_rate: learning rate of optimizer\n",
    "        :param z_input_dim: input dim of z\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.z_input_dim = z_input_dim\n",
    "        self.D = self.discriminator()\n",
    "        self.G = self.generator()\n",
    "        self.GD = self.combined()\n",
    "\n",
    "    def discriminator(self):\n",
    "        \"\"\"\n",
    "        define discriminator\n",
    "        \"\"\"\n",
    "        D = Sequential()\n",
    "        D.add(\n",
    "            Conv2D(256, (5, 5),\n",
    "                   padding='same',\n",
    "                   input_shape=(1, 28, 28),\n",
    "                   kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "        D.add(LeakyReLU(0.2))\n",
    "        D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        D.add(Dropout(0.3))\n",
    "        D.add(Conv2D(512, (5, 5), padding='same'))\n",
    "        D.add(LeakyReLU(0.2))\n",
    "        D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        D.add(Dropout(0.3))\n",
    "        D.add(Flatten())\n",
    "        D.add(Dense(256))\n",
    "        D.add(LeakyReLU(0.2))\n",
    "        D.add(Dropout(0.3))\n",
    "        D.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n",
    "        D.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "        return D\n",
    "\n",
    "    def generator(self):\n",
    "        \"\"\"\n",
    "        define generator\n",
    "        \"\"\"\n",
    "        G = Sequential()\n",
    "        G.add(Dense(512, input_dim=self.z_input_dim))\n",
    "        G.add(LeakyReLU(0.2))\n",
    "        G.add(Dense(128 * 7 * 7))\n",
    "        G.add(LeakyReLU(0.2))\n",
    "        G.add(BatchNormalization())\n",
    "        G.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7, )))\n",
    "        G.add(UpSampling2D(size=(2, 2)))\n",
    "        G.add(Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
    "        G.add(UpSampling2D(size=(2, 2)))\n",
    "        G.add(Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n",
    "        G.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "        return G\n",
    "\n",
    "    def combined(self):\n",
    "        \"\"\"\n",
    "        defien combined gan model\n",
    "        \"\"\"\n",
    "        G, D = self.G, self.D\n",
    "        D.trainable = False\n",
    "        GD = Sequential()\n",
    "        GD.add(G)\n",
    "        GD.add(D)\n",
    "\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n",
    "        GD.compile(loss='binary_crossentropy',\n",
    "                   optimizer=adam,\n",
    "                   metrics=['accuracy'])\n",
    "        D.trainable = True\n",
    "        return GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjzV3EMG7J91"
   },
   "source": [
    "## Learner 구성\n",
    "\n",
    "- discriminator와 generator를 다른 epoch 비율로 학습 가능하도록 구성\n",
    "- 20 epoch 마다 이미지 생성\n",
    "- D, G를 각각 학습\n",
    "- 학습 완료 후 loss graph 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.404436Z",
     "start_time": "2021-05-19T15:32:56.426Z"
    },
    "id": "UcZP9H6b7FFF"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, batch_size, epochs, learning_rate, z_input_dim,\n",
    "                 n_iter_D, n_iter_G):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.z_input_dim = z_input_dim\n",
    "        self.data = Data(self.batch_size, self.z_input_dim)\n",
    "\n",
    "        # the reason why D, G differ in iter : Generator needs more training than Discriminator\n",
    "        self.n_iter_D = n_iter_D\n",
    "        self.n_iter_G = n_iter_G\n",
    "        self.gan = GAN(self.learning_rate, self.z_input_dim)\n",
    "\n",
    "        # print status\n",
    "        batch_count = self.data.x_data.shape[0] / self.batch_size\n",
    "        print('Epochs:', self.epochs)\n",
    "        print('Batch size:', self.batch_size)\n",
    "        print('Batches per epoch:', batch_count)\n",
    "        print('Learning rate:', self.learning_rate)\n",
    "        print('Image data format:', K.image_data_format())\n",
    "\n",
    "    def fit(self):\n",
    "        self.d_loss = []\n",
    "        self.g_loss = []\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # train discriminator by real data\n",
    "            dloss = 0\n",
    "            for iter in range(self.n_iter_D):\n",
    "                dloss = self.train_D()\n",
    "\n",
    "            # train GD by generated fake data\n",
    "            gloss = 0\n",
    "            for iter in range(self.n_iter_G):\n",
    "                gloss = self.train_G()\n",
    "\n",
    "            # save loss data\n",
    "            self.d_loss.append(dloss)\n",
    "            self.g_loss.append(gloss)\n",
    "\n",
    "            # plot and save model each 20n epoch\n",
    "            if epoch % 20 == 0:\n",
    "                self.plot_generate_images(epoch, self.gan.G, examples=8)\n",
    "                print('Epoch:', str(epoch))\n",
    "                print('Discriminator loss:', str(dloss))\n",
    "                print('Generator loss:', str(gloss))\n",
    "\n",
    "        # show loss after train\n",
    "        self.plot_loss_graph(self.g_loss, self.d_loss)\n",
    "\n",
    "    def train_D(self):\n",
    "        \"\"\"\n",
    "        train Discriminator\n",
    "        \"\"\"\n",
    "\n",
    "        # Real data\n",
    "        real = self.data.get_real_sample()\n",
    "\n",
    "        # Generated data\n",
    "        z = self.data.get_z_sample(self.batch_size)\n",
    "        generated_images = self.gan.G.predict(z)\n",
    "\n",
    "        # labeling and concat generated, real images\n",
    "        x = np.concatenate((real, generated_images), axis=0)\n",
    "        y = [0.9] * self.batch_size + [0] * self.batch_size\n",
    "\n",
    "        # train discriminator\n",
    "        self.gan.D.trainable = True\n",
    "        loss = self.gan.D.train_on_batch(x, y)\n",
    "        return loss\n",
    "\n",
    "    def train_G(self):\n",
    "        \"\"\"\n",
    "        train Generator\n",
    "        \"\"\"\n",
    "\n",
    "        # Generated data\n",
    "        z = self.data.get_z_sample(self.batch_size)\n",
    "\n",
    "        # labeling\n",
    "        y = [1] * self.batch_size\n",
    "\n",
    "        # train generator\n",
    "        self.gan.D.trainable = False\n",
    "        loss = self.gan.GD.train_on_batch(z, y)\n",
    "        return loss\n",
    "\n",
    "    def plot_loss_graph(self, g_loss, d_loss):\n",
    "        \"\"\"\n",
    "        Save training loss graph\n",
    "        \"\"\"\n",
    "\n",
    "        # show loss graph\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(d_loss, label='Discriminator loss')\n",
    "        plt.plot(g_loss, label='Generator loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_generate_images(self, epoch, generator, examples=8):\n",
    "        \"\"\"\n",
    "        Save generated mnist images\n",
    "        \"\"\"\n",
    "        # plt info\n",
    "        dim = (10, 10)\n",
    "        figsize = (10, 10)\n",
    "\n",
    "        # generate images\n",
    "        z = self.data.get_z_sample(examples)\n",
    "        generated_images = generator.predict(z)\n",
    "\n",
    "        # show images\n",
    "        plt.figure(figsize=figsize)\n",
    "        for i in range(generated_images.shape[0]):\n",
    "            plt.subplot(dim[0], dim[1], i + 1)\n",
    "            plt.imshow(generated_images[i].reshape((28, 28)),\n",
    "                       interpolation='nearest',\n",
    "                       cmap='gray_r')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnwetMR87-Za"
   },
   "source": [
    "## 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.405435Z",
     "start_time": "2021-05-19T15:32:56.429Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7807
    },
    "id": "U2mIyr0c75-Y",
    "outputId": "1136e27e-6f75-4315-ff4f-264ce09c9745"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # set hyper parameters\n",
    "    batch_size = 128\n",
    "    epochs = 1000\n",
    "    learning_rate = 0.0002\n",
    "    z_input_dim = 100\n",
    "    n_iter_D = 1\n",
    "    n_iter_G = 5\n",
    "\n",
    "    # run model\n",
    "    model = Model(batch_size, epochs, learning_rate, z_input_dim, n_iter_D,\n",
    "                  n_iter_G)\n",
    "    model.fit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVhN5VeLCf9w"
   },
   "source": [
    "## 모델 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.405435Z",
     "start_time": "2021-05-19T15:32:56.431Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2985
    },
    "id": "WUPFmJsp8AFj",
    "outputId": "f2d7f40e-2c48-4152-f2e3-aa881212361d"
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.406433Z",
     "start_time": "2021-05-19T15:32:56.434Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1532
    },
    "id": "6b8MYOq9BduD",
    "outputId": "bca5818c-42a6-4137-8cdb-b23b20ee646a"
   },
   "outputs": [],
   "source": [
    "D = Sequential()\n",
    "D.add(Conv2D(256, (5, 5),\n",
    "         padding='same',\n",
    "         input_shape=(1, 28, 28),\n",
    "         kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "D.add(LeakyReLU(0.2))\n",
    "D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "D.add(Dropout(0.3))\n",
    "D.add(Conv2D(512, (5, 5), padding='same'))\n",
    "D.add(LeakyReLU(0.2))\n",
    "D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "D.add(Dropout(0.3))\n",
    "D.add(Flatten())\n",
    "D.add(Dense(256))\n",
    "D.add(LeakyReLU(0.2))\n",
    "D.add(Dropout(0.3))\n",
    "D.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "D.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(D, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T15:32:59.407429Z",
     "start_time": "2021-05-19T15:32:56.436Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1200
    },
    "id": "VHFXDUjABono",
    "outputId": "6b9573c7-f906-4264-85ec-cd74c825d446"
   },
   "outputs": [],
   "source": [
    "G = Sequential()\n",
    "G.add(Dense(512, input_dim=100))\n",
    "G.add(LeakyReLU(0.2))\n",
    "G.add(Dense(128 * 7 * 7))\n",
    "G.add(LeakyReLU(0.2))\n",
    "G.add(BatchNormalization())\n",
    "G.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7, )))\n",
    "G.add(UpSampling2D(size=(2, 2)))\n",
    "G.add(Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
    "G.add(UpSampling2D(size=(2, 2)))\n",
    "G.add(Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "G.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(G, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "keras-gan-mnist-tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
